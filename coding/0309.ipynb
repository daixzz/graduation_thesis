{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# 其中的部分数据\n",
    "filename = 'data1/4th138.json'\n",
    "data = open(filename, encoding='utf-8')\n",
    "strJson1 = json.load(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36\"\n",
    "        \"Referer\": \"https://weibo.com\"\n",
    "    }\n",
    "    cookies = {\n",
    "        'cookie':'SINAGLOBAL=4954692280506.27.1591184066784; UOR=,,www.baidu.com; SCF=Au_QDngCAtZdjo2U3MOWDe9AqNjD2bv9bCKkhre3FM48FGdJz4rKLJEWX6ecNp7q5Tg7oJ6FfSgy02zp5_XMbKg.; ULV=1646568933516:68:1:1:5347983324126.591.1646568933470:1644499064874; SUB=_2A25PLLcYDeRhGeVO6VQV8irKyzmIHXVsW6_QrDV8PUNbmtANLRHBkW9NTWqL0HM9wbgK-_eNB3WwaFemzOLZknK_; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFd.fwHbnebHoYw4M0SwxAm5JpX5KzhUgL.Foe7eoqXeoBceh-2dJLoI7__UgSQIs8DMJxRwntt; ALF=1678375623; SSOLoginState=1646839624'\n",
    "        #'cookie':'SINAGLOBAL=4954692280506.27.1591184066784; UOR=,,www.baidu.com; ULV=1640670361949:63:14:1:8632450552135.697.1640670361940:1639835996800; SCF=Au_QDngCAtZdjo2U3MOWDe9AqNjD2bv9bCKkhre3FM48Gh6mDFxMCacqDmefa19z_7fYJ9W_ldjCbGt1vKO7CaI.; SUB=_2A25Mz7rVDeRhGeVO6VQV8irKyzmIHXVvvKsdrDV8PUJbmtAKLRn-kW9NTWqL0FkKERyR5ZWAAhKgqSgGF5QPtnkp; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFd.fwHbnebHoYw4M0SwxAm5JpX5K-hUgL.Foe7eoqXeoBceh-2dJLoI7__UgSQIs8DMJxRwntt; ALF=1672280760; SSOLoginState=1640745605; XSRF-TOKEN=8rdPjDQ-y5jyz0mWRw5ZLY24'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, cookies=cookies)\n",
    "    time.sleep(3)   # 加上5s 的延时防止被反爬\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"ok\":0,\"msg\":\"\\\\u8fd9\\\\u91cc\\\\u8fd8\\\\u6ca1\\\\u6709\\\\u5185\\\\u5bb9\",\"data\":{\"cards\":[]}}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "header = {\n",
    "    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
    "        }\n",
    "uid = '1226258061'\n",
    "page = 0\n",
    "url='https://m.weibo.cn/api/container/getIndex?uid={}&t=0&luicode=10000011&lfid=100103type%3D1%26q%3D%E5%9B%9B%E5%B7%9D%E6%97%A5%E6%8A%A5&type=uid&value={}&containerid=107603{}&page={}'.format(uid,uid,uid,page)\n",
    "URL = 'https://m.weibo.cn/api/container/getIndex?uid=3026424605&luicode=10000011&lfid=231093_-_selffollowed&type=uid&value=1222425514&containerid=1076031222425514'\n",
    "html = requests.get(url = URL, headers = header)\n",
    "html.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬取微博ID\n",
      "爬取失败\n",
      "爬取失败\n",
      "爬取失败\n",
      "爬取失败\n",
      "爬取失败\n",
      "[]\n",
      "每条微博的text爬取：\n",
      "爬取微博个数：0\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5a2538494a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mmakeWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-5a2538494a3f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclearText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mmakeWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-5a2538494a3f>\u001b[0m in \u001b[0;36mmakeWordCloud\u001b[0;34m(TEXT)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mcut_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#jieba分词，空格字符分隔\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     wordcloud = WordCloud(\n\u001b[0m\u001b[1;32m     79\u001b[0m        \u001b[0;31m#设置字体，不然会出现口字乱码，文字的路径是电脑的字体一般路径，可以换成别的\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m        \u001b[0mfont_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C:/Windows/Fonts/simfang.ttf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \"\"\"\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \"\"\"\n\u001b[1;32m    613\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0m\u001b[1;32m    404\u001b[0m                              \"got %d.\" % len(frequencies))\n\u001b[1;32m    405\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt  #绘制图像的模块\n",
    "import  jieba                    #jieba分词\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #解决中文乱码\n",
    "\n",
    "'''\n",
    "~总思路：（找接口是真的麻烦）\n",
    "0.网页版微博找了半天没找到接口，在移动端可以找到，移动端微博：https://m.weibo.cn/\n",
    "1.Chrome浏览器的开发者工具下找到getIndex?网址。\n",
    "2.先爬取微博ID存入列表。\n",
    "3.然后找到每个ID相对应的微博url，爬取微博内容和发布时间。\n",
    "4.清理数据，只剩下中文。\n",
    "5.利用jieba分词。\n",
    "6.画出词云。\n",
    "'''\n",
    "'''\n",
    "函数说明：\n",
    "getID():爬取微博ID\n",
    "getText():爬取微博的文本即发表时间\n",
    "clearText():清理文本，只保留中文\n",
    "makeWordCloud():分词，绘图\n",
    "'''\n",
    "def getID(ID):\n",
    "    print(\"爬取微博ID\")\n",
    "#    ID = []\n",
    "    for p in range(0,5): #自己设置页数\n",
    "        #经过尝试，在网址后面加“&page=”可以翻页\n",
    "        URL = \"https://m.weibo.cn/api/container/getIndex?uid=3026424605&luicode=10000011&lfid=231093_-_selffollowed&type=uid&value=1222425514&containerid=1076031222425514\"+\"&page=\"+str(p)\n",
    "        try:    \n",
    "            r = requests.get(url = URL, headers = header)        \n",
    "            r.raise_for_status()\n",
    "            r.encoding = r.apparent_encoding\n",
    "            IDD = re.findall('\"id\":\"(.*?)\"',r.text) #匹配id，IDD为当前页面的所有id\n",
    "            ID += IDD #将当前页面的id加入到总ID列表\n",
    "            print('爬取到的id为'+ ID)\n",
    "        except:\n",
    "            print(\"爬取失败\")   \n",
    "    print(ID)\n",
    "    return ID\n",
    "\n",
    "def getText(ID, text):\n",
    "    print(\"每条微博的text爬取：\")\n",
    "    text = []\n",
    "    textsingle = []\n",
    "    for i in range(len(ID)):\n",
    "        print(\"ID:\"+ID[i])\n",
    "        #url后加微博ID可查看此微博的信息，其中text为文本，created_at为发表时间\n",
    "        url = \"https://m.weibo.cn/detail/\"+ID[i]\n",
    "        print(url)\n",
    "        try:    \n",
    "            r = requests.get(url = url, headers = header)        \n",
    "            r.raise_for_status()\n",
    "            r.encoding = r.apparent_encoding\n",
    "            createTime = re.findall('\"created_at\": \"(.*?)\"',r.text) #匹配发表时间\n",
    "            print(\"create at:\"+createTime[0])\n",
    "            print(\"~ ~ ~ ~ ~ ~ ~\")\n",
    "            textsingle = re.findall('\"text\": \"(.*?)\"',r.text) #匹配此微博文本\n",
    "            text += textsingle #将此微博文本加入总微博文本列表\n",
    "        except:\n",
    "            print(\"爬取失败\")\n",
    "    TEXT = ''\n",
    "    for i in range(len(text)):\n",
    "        TEXT = TEXT + text[i] #将列表转换成字符型TEXT，便于分词\n",
    "    print(\"爬取微博个数：\"+str(len(ID)))\n",
    "    return TEXT\n",
    "\n",
    "def clearText(TEXT):\n",
    "    rule = re.compile(u\"[^\\u4e00-\\u9fa5]\") #匹配非中文字符\n",
    "    TEXT = rule.sub('',TEXT) #将非中文字符替换为空\n",
    "    return TEXT\n",
    "\n",
    "def makeWordCloud(TEXT):    \n",
    "    cut_text = \" \".join(jieba.cut(TEXT)) #jieba分词，空格字符分隔\n",
    "    print(cut_text)\n",
    "    wordcloud = WordCloud(\n",
    "       #设置字体，不然会出现口字乱码，文字的路径是电脑的字体一般路径，可以换成别的\n",
    "       font_path=\"C:/Windows/Fonts/simfang.ttf\",\n",
    "       #设置了背景，宽高\n",
    "       background_color=\"white\",width=1000,height=880).generate(cut_text)\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    header = {\n",
    "        'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
    "        }\n",
    "    ID = []\n",
    "    text = []\n",
    "    ID = getID(ID)\n",
    "    text = getText(ID,text)\n",
    "    text = clearText(text)\n",
    "    makeWordCloud(text)\n",
    "    \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'm.weibo.cn'\n",
    "base_url = 'https://%s/api/container/getIndex?' % host\n",
    "user_agent = 'User-Agent: Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13B143 Safari/601.1 wechatdevtools/0.7.0 MicroMessenger/6.3.9 Language/zh_CN webview/0'#这里的user_agent是网上找的\n",
    "\n",
    "user_id = str(3026424605)#这串数字就是用户id\n",
    "headers = {\n",
    "    'Host': host,\n",
    "    'Referer': 'https://m.weibo.cn/u/%s'%user_id,\n",
    "    'User-Agent': user_agent\n",
    "}\n",
    "import datetime\n",
    "def timestr_standard(time_str):\n",
    "    now_time = datetime.datetime.now()\n",
    "    if time_str.endswith('分钟前') or time_str.endswith('小时前') or time_str == '刚刚':\n",
    "        #strptime是把字符串转换为时间类。strftime是把时间转换为字符串\n",
    "        time_standard = datetime.datetime.strftime(now_time.date(),'%Y-%m-%d')\n",
    "    elif time_str.startswith('昨天'):\n",
    "        time_standard = datetime.datetime.strftime((now_time - datetime.timedelta(days = 1)).date(),'%Y-%m-%d')\n",
    "    elif time_str.startswith('0') or time_str.startswith('1'):\n",
    "        time_standard = str(now_time.year) + '-' + time_str\n",
    "    elif time_str.startswith('20'):\n",
    "        time_standard = time_str\n",
    "    return time_standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "import os\n",
    "#import time_standard as tst\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlencode\n",
    "from pyquery import PyQuery as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_page(page):\n",
    "    params = {\n",
    "        'type': 'uid',\n",
    "        'value': 1665372775,\n",
    "        'containerid': int('107603' + user_id),#containerid就是微博用户id前面加上107603\n",
    "        'page': page\n",
    "    }\n",
    "    url = base_url + urlencode(params)\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "    except requests.ConnectionError as e:\n",
    "        print('抓取错误', e.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析页面返回的json数据\n",
    "def analysis_page(json):#保存图片的文件夹路径\n",
    "    items = json.get('data').get('cards')\n",
    "    for item in items:\n",
    "        item = item.get('mblog')\n",
    "        if item:\n",
    "            data = {\n",
    "                #'created_at': item.get('created_at'),#微博创建日期\n",
    "                'text': pq(item.get(\"text\")).text(),  # 仅提取内容中的文本\n",
    "                #'attitudes': item.get('attitudes_count'),#点赞数\n",
    "                #'comments': item.get('comments_count'),#评论数\n",
    "                #'reposts': item.get('reposts_count')#转发数\n",
    "            }\n",
    "            base_data[len(base_data)] = data#把得到的数据字典存入总字典\n",
    "#             if pic_choice == 'y':#如果选择保存图片\n",
    "#                 pics = item.get('pics')\n",
    "#                 if pics:\n",
    "#                     for pic in pics:\n",
    "#                         picture_url = pic.get('large').get('url')#得到原图地址\n",
    "#                         pid = pic.get('pid')#图片id\n",
    "#                         pic_name = tst.timestr_standard(data['created_at']) + '_' + pid[25:]#构建保存图片文件名，timestr_standard是一个把微博的created_at字符串转换为‘XXXX-XX-XX’形式日期的一个函数\n",
    "#                         download_pics(picture_url,pic_name,pic_filebagPath)#下载原图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " totally cost -1932.026831150055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    base_data = {}\n",
    "    #page = input('请输入你要爬取的页数')#可输入爬取页数，或者输入‘all’爬取所有微博\n",
    "    page = 5\n",
    "    #pic_choice = input('是否需要存储图片?y/n')#选择是否保存图片\n",
    "    pic_choice = 'n'\n",
    "    time_start=time.time()\n",
    "    try:\n",
    "        json = get_single_page(1)\n",
    "        screen_name = json.get('data').get('cards')[0].get('mblog').get('user').get('screen_name')#博主昵称\n",
    "        total = json.get('data').get('cardlistInfo').get('total')#博主微博总条数\n",
    "#         if pic_choice == 'y':#如果选择保存图片，则分配图片保存路径\n",
    "#             pic_filebagPath = 'D:\\\\python_project\\\\crawl\\\\weibo\\\\%s_picture'%screen_name\n",
    "#             os.makedirs(pic_filebagPath)#建立文件夹\n",
    "#         else:\n",
    "#             pic_filebagPath = None#选择不保存文件夹则不分配路径\n",
    "#         if page == 'all':#寻找总条数\n",
    "#             page = total//10\n",
    "#             while get_single_page(page).get('ok') == 1:\n",
    "#                 page = page + 1\n",
    "#             print('总页数为：%s'%page)\n",
    "        page = int(page) + 1\n",
    "        for page in tqdm(range(1,page)):  # 抓取数据\n",
    "            json = get_single_page(page)\n",
    "            analysis_page(json)\n",
    "    except Exception as e:\n",
    "        print('error:',e)\n",
    "    finally:\n",
    "        #base_dataPath = 'D:\\\\python_project\\\\crawl\\\\weibo\\\\base_data_%s.txt'%screen_name#base_data保存地址和文件名\n",
    "        #f = open(base_dataPath,'w+',encoding='utf-8')\n",
    "        #f.write(str(base_data))\n",
    "        #f.close()\n",
    "        #time_end=time.time()\n",
    "        print('\\n totally cost',time_end-time_start)#显示程序运行时间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'text': '极端分子出没 查看图片'},\n",
       " 1: {'text': '诡计多端的外企 三八就你不放假 礼物是小免洗洗手液～'},\n",
       " 2: {'text': '毕业论文 我真的交不出来 救救偶'},\n",
       " 3: {'text': 'Repost'},\n",
       " 4: {'text': '如果我有钱了…'},\n",
       " 5: {'text': '今天一天都会心碎复健'},\n",
       " 6: {'text': 'Repost'},\n",
       " 7: {'text': '支持'},\n",
       " 8: {'text': '谁呀 新年第一天 狠狠的e了'},\n",
       " 9: {'text': '我觉得差不多了 大概就是以前觉得托福过了gre差不多了就是稳稳的幸福 但是现在审视整个申请过程 真正拖后腿的其实是本科院校和gpa 现在颇有少壮不努力老大徒伤悲的感觉'},\n",
       " 10: {'text': '原来在水逆mdmdmdmdmd'},\n",
       " 11: {'text': '左思右想还是觉得今年这个年开得有问题'},\n",
       " 12: {'text': '5555如果某个学校不能update托福成绩 那我一月份考他的意义在哪'},\n",
       " 13: {'text': '想到还有几个学校的推荐信链接没有发给老师 我一整个就是不想申请了'},\n",
       " 14: {'text': '20岁离开得太快 对2021年的记忆居然只剩下了最后几个月不停考试的崩溃21岁生活\\n2021没有那么糟糕尼'},\n",
       " 15: {'text': '@ets 出下我今年的年度报告'},\n",
       " 16: {'text': '求求'},\n",
       " 17: {'text': '转发微博'},\n",
       " 18: {'text': 'ok'},\n",
       " 19: {'text': '球球了//@之之無語:🙏🙏🙏'},\n",
       " 20: {'text': '球球//@muchhairdxz:我不保证了//@muchhairdxz:信女愿两个月不说脏话//@muchhairdxz:🙏 信女愿一个月不骂脏话'},\n",
       " 21: {'text': '转发微博'},\n",
       " 22: {'text': '我重新拼100次offer 弥补我的错误 对不起offer之神'},\n",
       " 23: {'text': '我是中国人 我不做分数的奴隶'},\n",
       " 24: {'text': '谢谢你'},\n",
       " 25: {'text': '我不保证了//@muchhairdxz:信女愿两个月不说脏话//@muchhairdxz:🙏 信女愿一个月不骂脏话'},\n",
       " 26: {'text': '笑死//@当代新锐男大学生:leecode，北美亚男的五三 //@不要再为难我啦:去了美国没混好，归因于去的是东岸NY不是西岸LA，去了西岸没混好，归因于leetcode题刷少了'},\n",
       " 27: {'text': '成不成的沉没成本都摆在那了 无所谓了'},\n",
       " 28: {'text': '生动形象的描述了我本人'},\n",
       " 29: {'text': '别人都在收offer 定好要去哪了 而我到现在只递交了一个申请 还是那种不要ps 不要推荐信的 意思就是ps cv rl我都还没整完'},\n",
       " 30: {'text': 'yes'},\n",
       " 31: {'text': '信女愿两个月不说脏话//@muchhairdxz:🙏 信女愿一个月不骂脏话'},\n",
       " 32: {'text': '谁不想美美躺平'},\n",
       " 33: {'text': '最好是'},\n",
       " 34: {'text': '🙏 信女愿一个月不骂脏话'},\n",
       " 35: {'text': 'yesok'},\n",
       " 36: {'text': '谢谢你奥'},\n",
       " 37: {'text': '莫拉古？？记者你但凡提前报道两天'},\n",
       " 38: {'text': '是真的'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
